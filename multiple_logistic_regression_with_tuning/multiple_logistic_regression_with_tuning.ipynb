{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDVDq4R4cQqN"
      },
      "source": [
        "NOTE: DO NOT SUBMIT THIS NOTEBOOK FOR YOUR SUBMISSION!!!\n",
        "PLEASE SUBMIT \"A3_submission.py\" after you have finished debugging.\n",
        "\n",
        "Import and setup some auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHPwL1QYcQqU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "import timeit\n",
        "from collections import OrderedDict\n",
        "from pprint import pformat\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "def compute_score(acc, min_thres, max_thres):\n",
        "    if acc <= min_thres:\n",
        "        base_score = 0.0\n",
        "    elif acc >= max_thres:\n",
        "        base_score = 100.0\n",
        "    else:\n",
        "        base_score = float(acc - min_thres) / (max_thres - min_thres) \\\n",
        "                     * 100\n",
        "    return base_score\n",
        "\n",
        "\n",
        "def run(algorithm, dataset_name, filename):\n",
        "    start = timeit.default_timer()\n",
        "    predicted_test_labels, gt_labels = algorithm(dataset_name)\n",
        "    if predicted_test_labels is None or gt_labels is None:\n",
        "      return (0, 0, 0)\n",
        "    stop = timeit.default_timer()\n",
        "    run_time = stop - start\n",
        "    \n",
        "    np.savetxt(filename, np.asarray(predicted_test_labels))\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for label, prediction in zip(gt_labels, predicted_test_labels):\n",
        "      total += label.size(0)\n",
        "      correct += (prediction.cpu().numpy() == label.cpu().numpy()).sum().item()   # assuming your model runs on GPU\n",
        "      \n",
        "    accuracy = float(correct) / total\n",
        "    \n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "    return (correct, accuracy, run_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3VfLcA4cQqx"
      },
      "source": [
        "TODO: Implement Logistic Regression here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17Mjmw05cQq0"
      },
      "outputs": [],
      "source": [
        "def logistic_regression(dataset_name):\n",
        "\n",
        "    n_epochs = 1\n",
        "    batch_size_train = 128\n",
        "    batch_size_test = 1000\n",
        "    learning_rate = 0.001\n",
        "    log_interval = 100\n",
        "    lam = 0.005\n",
        "    ceLoss = nn.CrossEntropyLoss()\n",
        "\n",
        "    # predictions and ground truth to be returned\n",
        "    preds = None\n",
        "    gts = None\n",
        "\n",
        "    # Checking GPU availability\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    # Multiple Logistic regression for MNIST\n",
        "    class MultipleLogisticRegression1(nn.Module):\n",
        "\n",
        "        def __init__(self):\n",
        "            super(MultipleLogisticRegression1, self).__init__()\n",
        "            self.fc = nn.Linear(28*28, 10)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.fc(x)\n",
        "            return x\n",
        "\n",
        "    # Multiple Logistic regression for CIFAR10\n",
        "\n",
        "    class MultipleLogisticRegression2(nn.Module):\n",
        "\n",
        "        def __init__(self):\n",
        "            super(MultipleLogisticRegression2, self).__init__()\n",
        "            self.fc = nn.Linear(3*32*32, 10)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.fc(x)\n",
        "            return x\n",
        "\n",
        "    # Following code appears at:  https://lirnli.wordpress.com/2017/09/03/one-hot-encoding-in-pytorch/\n",
        "\n",
        "    class One_Hot(nn.Module):\n",
        "        def __init__(self, depth):\n",
        "            super(One_Hot, self).__init__()\n",
        "            self.depth = depth\n",
        "            self.ones = torch.sparse.torch.eye(depth).to(device)\n",
        "\n",
        "        def forward(self, X_in):\n",
        "            X_in = X_in.long()\n",
        "            return self.ones.index_select(0, X_in.data)\n",
        "\n",
        "        def __repr__(self):\n",
        "            return self.__class__.__name__ + \"({})\".format(self.depth)\n",
        "\n",
        "    if dataset_name == \"MNIST\":\n",
        "\n",
        "        MNIST_training = torchvision.datasets.MNIST('/MNIST_dataset/', train=True, download=True,\n",
        "                                                    transform=torchvision.transforms.Compose([\n",
        "                                                        torchvision.transforms.ToTensor(),\n",
        "                                                        torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "        MNIST_test_set = torchvision.datasets.MNIST('/MNIST_dataset/', train=False, download=True,\n",
        "                                                    transform=torchvision.transforms.Compose([\n",
        "                                                        torchvision.transforms.ToTensor(),\n",
        "                                                        torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "        # create a training and a validation set\n",
        "        MNIST_training_set, MNIST_validation_set = random_split(\n",
        "            MNIST_training, [48000, 12000])\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            MNIST_training_set, batch_size=batch_size_train, shuffle=True)\n",
        "        validation_loader = torch.utils.data.DataLoader(\n",
        "            MNIST_validation_set, batch_size=batch_size_train, shuffle=True)\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            MNIST_test_set, batch_size=batch_size_test, shuffle=True)\n",
        "\n",
        "        # setup multi-logistic_model, optimizer, and onehot\n",
        "        multi_logistic_model = MultipleLogisticRegression1().to(device)\n",
        "        optimizer = optim.Adam(\n",
        "            multi_logistic_model.parameters(), lr=learning_rate)\n",
        "        one_hot = One_Hot(10).to(device)\n",
        "\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "\n",
        "        # Source: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "        CIFAR10_training = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                        download=True, transform=transform)\n",
        "        CIFAR10_test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                                        download=True, transform=transform)\n",
        "\n",
        "        # create a training and a validation set\n",
        "        CIFAR10_training_set, CIFAR10_validation_set = random_split(\n",
        "            CIFAR10_training, [38000, 12000])\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = torch.utils.data.DataLoader(CIFAR10_training_set,\n",
        "                                                   batch_size=batch_size_train,\n",
        "                                                   shuffle=True, num_workers=2)\n",
        "        validation_loader = torch.utils.data.DataLoader(CIFAR10_validation_set,\n",
        "                                                        batch_size=batch_size_train,\n",
        "                                                        shuffle=True, num_workers=2)\n",
        "        test_loader = torch.utils.data.DataLoader(CIFAR10_test_set,\n",
        "                                                  batch_size=batch_size_test,\n",
        "                                                  shuffle=False, num_workers=2)\n",
        "\n",
        "        multi_logistic_model = MultipleLogisticRegression2().to(device)\n",
        "        optimizer = optim.Adam(\n",
        "            multi_logistic_model.parameters(), lr=learning_rate)\n",
        "        one_hot = One_Hot(10).to(device)\n",
        "\n",
        "    # Training multi_logistic_model\n",
        "\n",
        "    def train(epoch):\n",
        "        multi_logistic_model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = multi_logistic_model(data)\n",
        "            loss = ceLoss(output, one_hot(target)) + lam * torch.sum(\n",
        "                torch.sum(torch.pow(list(multi_logistic_model.parameters())[0], 2)), 0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "    # Validating multi_logistic_model\n",
        "    def validation():\n",
        "        multi_logistic_model.eval()\n",
        "        validation_loss = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():  # notice the use of no_grad\n",
        "            for data, target in validation_loader:\n",
        "                data = data.to(device)\n",
        "                target = target.to(device)\n",
        "                output = multi_logistic_model(data)\n",
        "                pred = output.data.max(1, keepdim=True)[1]\n",
        "                correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "                validation_loss += ceLoss(output, one_hot(target)).item() + lam * torch.sum(\n",
        "                    torch.sum(torch.pow(list(multi_logistic_model.parameters())[0], 2)), 0).item()\n",
        "        validation_loss = validation_loss / \\\n",
        "            len(validation_loader.dataset) * batch_size_train\n",
        "        print('\\nValidation set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            validation_loss, correct, len(validation_loader.dataset), 100. * correct / len(validation_loader.dataset)))\n",
        "\n",
        "    # Validation benchmark\n",
        "    validation()\n",
        "\n",
        "    # Go through epoch range(1, n_epochs + 1)\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train(epoch)\n",
        "        validation()\n",
        "\n",
        "    multi_logistic_model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = multi_logistic_model(data)\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            if count == 0:\n",
        "                preds = pred\n",
        "                gts = target.data.view_as(pred)\n",
        "            else:\n",
        "                preds = torch.cat((preds, pred), 0)\n",
        "                gts = torch.cat((gts, target.data.view_as(pred)), 0)\n",
        "            count += 1\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "            test_loss += ceLoss(output, one_hot(target)).item() + lam * torch.sum(\n",
        "                torch.sum(torch.pow(list(multi_logistic_model.parameters())[0], 2)), 0).item()\n",
        "\n",
        "    test_loss = test_loss / len(test_loader.dataset) * batch_size_test\n",
        "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    preds = preds.to(\"cpu\")\n",
        "    gts = gts.to(\"cpu\")\n",
        "\n",
        "    return preds, gts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b0wckOjLdpo"
      },
      "source": [
        "TODO: Implement Hyper-parameter Tuning here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTI1zzo9Ldpo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tune_hyper_parameter():\n",
        "\n",
        "    n_epochs = 1\n",
        "    batch_size_train = 128\n",
        "    batch_size_test = 1000\n",
        "    log_interval = 100\n",
        "    ceLoss = nn.CrossEntropyLoss()\n",
        "\n",
        "    # predictions and ground truth to be returned\n",
        "    preds = None\n",
        "    gts = None\n",
        "\n",
        "    # Checking GPU availability\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    # Multiple logistic regression for CIFAR10\n",
        "\n",
        "    class MultipleLogisticRegression2(nn.Module):\n",
        "\n",
        "        def __init__(self):\n",
        "            super(MultipleLogisticRegression2, self).__init__()\n",
        "            self.fc = nn.Linear(3*32*32, 10)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.fc(x)\n",
        "            return x\n",
        "\n",
        "    # Following code appears at:  https://lirnli.wordpress.com/2017/09/03/one-hot-encoding-in-pytorch/\n",
        "    class One_Hot(nn.Module):\n",
        "        def __init__(self, depth):\n",
        "            super(One_Hot, self).__init__()\n",
        "            self.depth = depth\n",
        "            self.ones = torch.sparse.torch.eye(depth).to(device)\n",
        "\n",
        "        def forward(self, X_in):\n",
        "            X_in = X_in.long()\n",
        "            return self.ones.index_select(0, X_in.data)\n",
        "\n",
        "        def __repr__(self):\n",
        "            return self.__class__.__name__ + \"({})\".format(self.depth)\n",
        "\n",
        "    # Source: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    CIFAR10_training = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                    download=True, transform=transform)\n",
        "    CIFAR10_test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                                    download=True, transform=transform)\n",
        "\n",
        "    # create a training and a validation set\n",
        "    CIFAR10_training_set, CIFAR10_validation_set = random_split(\n",
        "        CIFAR10_training, [38000, 12000])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(CIFAR10_training_set,\n",
        "                                               batch_size=batch_size_train,\n",
        "                                               shuffle=True, num_workers=2)\n",
        "    validation_loader = torch.utils.data.DataLoader(CIFAR10_validation_set,\n",
        "                                                    batch_size=batch_size_train,\n",
        "                                                    shuffle=True, num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(CIFAR10_test_set,\n",
        "                                              batch_size=batch_size_test,\n",
        "                                              shuffle=False, num_workers=2)\n",
        "\n",
        "    multi_logistic_model = MultipleLogisticRegression2().to(device)\n",
        "    one_hot = One_Hot(10).to(device)\n",
        "\n",
        "    # Training multi_logistic_model\n",
        "    def train(epoch):\n",
        "        multi_logistic_model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = multi_logistic_model(data)\n",
        "            loss = ceLoss(output, one_hot(target)) + lam * torch.sum(\n",
        "                torch.sum(torch.pow(list(multi_logistic_model.parameters())[0], 2)), 0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "    # Validating multi_logistic_model\n",
        "    def validation():\n",
        "        multi_logistic_model.eval()\n",
        "        validation_loss = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():  # notice the use of no_grad\n",
        "            for data, target in validation_loader:\n",
        "                data = data.to(device)\n",
        "                target = target.to(device)\n",
        "                output = multi_logistic_model(data)\n",
        "                pred = output.data.max(1, keepdim=True)[1]\n",
        "                correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "                validation_loss += ceLoss(output, one_hot(target)).item() + lam * torch.sum(\n",
        "                    torch.sum(torch.pow(list(multi_logistic_model.parameters())[0], 2)), 0).item()\n",
        "        validation_loss = validation_loss / \\\n",
        "            len(validation_loader.dataset) * batch_size_train\n",
        "        print('\\nValidation set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            validation_loss, correct, len(validation_loader.dataset), 100. * correct / len(validation_loader.dataset)))\n",
        "        return 100. * correct / len(validation_loader.dataset)\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    accuracy = 0.0\n",
        "    best_lam_adam = 0.0\n",
        "    best_lam_sgd = 0.0\n",
        "    lam = 0.0\n",
        "    learning_rate = 0.0\n",
        "    best_learning_rate_adam = 0.0\n",
        "    best_learning_rate_sgd = 0.0\n",
        "\n",
        "    for i in [0.01, 0.005, 0.001]:\n",
        "        for j in [0.01, 0.005, 0.001]:\n",
        "            lam = i\n",
        "            learning_rate = j\n",
        "            optimizer = optim.Adam(\n",
        "                multi_logistic_model.parameters(), lr=learning_rate)\n",
        "            for epoch in range(1, n_epochs + 1):\n",
        "                train(epoch)\n",
        "            accuracy = validation().item()\n",
        "            print(\"Validation set accuracy: \", accuracy)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_lam_adam = lam\n",
        "                best_learning_rate_adam = learning_rate\n",
        "    print(\"Best lambda for adam: \", best_lam_adam)\n",
        "    print(\"Best learning rate for adam: \", best_learning_rate_adam)\n",
        "\n",
        "    for i in [0.01, 0.005, 0.001]:\n",
        "        for j in [0.01, 0.005, 0.001]:\n",
        "            lam = i\n",
        "            learning_rate = j\n",
        "            optimizer = torch.optim.SGD(\n",
        "                multi_logistic_model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "            for epoch in range(1, n_epochs + 1):\n",
        "                train(epoch)\n",
        "            accuracy = validation().item()\n",
        "            print(\"Validation set accuracy: \", accuracy)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_lam_sgd = lam\n",
        "                best_learning_rate_sgd = learning_rate\n",
        "    print(\"Best lambda for adam: \", best_lam_adam)\n",
        "    print(\"Best learning rate for adam: \", best_learning_rate_adam)\n",
        "    print(\"Best lambda for sgd : \", best_lam_sgd)\n",
        "    print(\"Best learning rate for sgd: \", best_learning_rate_sgd)\n",
        "\n",
        "    return best_lam_adam, best_learning_rate_adam, best_lam_sgd, best_learning_rate_sgd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNNgL7C7cQq-"
      },
      "source": [
        "Main loop. Run time and total score will be shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf9iL8S_cQrB",
        "outputId": "ad4fdd51-a614-468a-e0bd-21c5bd28c2a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "\n",
            "Validation set: Avg. loss: 2.4080, Accuracy: 1068/12000 (9%)\n",
            "\n",
            "Train Epoch: 1 [0/48000 (0%)]\tLoss: 2.450499\n",
            "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.441444\n",
            "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.345871\n",
            "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.379511\n",
            "\n",
            "Validation set: Avg. loss: 0.3787, Accuracy: 10863/12000 (91%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.3610, Accuracy: 9122/10000 (91%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 91 %\n",
            "cuda:0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Validation set: Avg. loss: 2.3468, Accuracy: 1310/12000 (11%)\n",
            "\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 2.321777\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.793432\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 1.775556\n",
            "\n",
            "Validation set: Avg. loss: 1.8273, Accuracy: 4680/12000 (39%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 1.8073, Accuracy: 3895/10000 (39%)\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 38 %\n",
            "\n",
            "Result:\n",
            " OrderedDict([   (   'MNIST',\n",
            "                    OrderedDict([   ('correct_predict', 9122),\n",
            "                                    ('accuracy', 0.9122),\n",
            "                                    ('score', 92.19999999999999),\n",
            "                                    ('run_time', 13.54629051400002)])),\n",
            "                (   'CIFAR10',\n",
            "                    OrderedDict([   ('correct_predict', 3895),\n",
            "                                    ('accuracy', 0.3895),\n",
            "                                    ('score', 100.0),\n",
            "                                    ('run_time', 15.928015622000032)])),\n",
            "                ('total_score', 96.1)])\n",
            "cuda:0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 2.393241\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 2.339498\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 2.428438\n",
            "\n",
            "Validation set: Avg. loss: 2.8028, Accuracy: 3245/12000 (27%)\n",
            "\n",
            "Validation set accuracy:  27.04166603088379\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 2.633550\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 2.215330\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 2.074884\n",
            "\n",
            "Validation set: Avg. loss: 2.1712, Accuracy: 3816/12000 (32%)\n",
            "\n",
            "Validation set accuracy:  31.799999237060547\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.951181\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.893109\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 1.846318\n",
            "\n",
            "Validation set: Avg. loss: 1.8569, Accuracy: 4493/12000 (37%)\n",
            "\n",
            "Validation set accuracy:  37.44166564941406\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.840212\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 2.853047\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 2.461528\n",
            "\n",
            "Validation set: Avg. loss: 2.8999, Accuracy: 3264/12000 (27%)\n",
            "\n",
            "Validation set accuracy:  27.19999885559082\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 2.568739\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.891548\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 2.118445\n",
            "\n",
            "Validation set: Avg. loss: 2.0431, Accuracy: 4143/12000 (35%)\n",
            "\n",
            "Validation set accuracy:  34.525001525878906\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.873181\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.832936\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 1.754926\n",
            "\n",
            "Validation set: Avg. loss: 1.8370, Accuracy: 4634/12000 (39%)\n",
            "\n",
            "Validation set accuracy:  38.61666488647461\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.534416\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 2.157183\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 2.486563\n",
            "\n",
            "Validation set: Avg. loss: 2.7126, Accuracy: 3767/12000 (31%)\n",
            "\n",
            "Validation set accuracy:  31.391666412353516\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 2.247169\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.834030\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 2.310973\n",
            "\n",
            "Validation set: Avg. loss: 2.1749, Accuracy: 3927/12000 (33%)\n",
            "\n",
            "Validation set accuracy:  32.724998474121094\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 2.118509\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.793494\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 1.911860\n",
            "\n",
            "Validation set: Avg. loss: 1.8311, Accuracy: 4578/12000 (38%)\n",
            "\n",
            "Validation set accuracy:  38.14999771118164\n",
            "Best lambda for adam:  0.005\n",
            "Best learning rate for adam:  0.001\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 2.050064\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.943901\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 2.036041\n",
            "\n",
            "Validation set: Avg. loss: 2.0032, Accuracy: 4379/12000 (36%)\n",
            "\n",
            "Validation set accuracy:  36.49166488647461\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.845203\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.839903\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 1.847460\n",
            "\n",
            "Validation set: Avg. loss: 1.8584, Accuracy: 4781/12000 (40%)\n",
            "\n",
            "Validation set accuracy:  39.84166717529297\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.858883\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.778009\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 1.652287\n",
            "\n",
            "Validation set: Avg. loss: 1.8261, Accuracy: 4849/12000 (40%)\n",
            "\n",
            "Validation set accuracy:  40.40833282470703\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.567206\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.699498\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 1.901894\n",
            "\n",
            "Validation set: Avg. loss: 1.8638, Accuracy: 4570/12000 (38%)\n",
            "\n",
            "Validation set accuracy:  38.08333206176758\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.743416\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.868387\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 1.811108\n",
            "\n",
            "Validation set: Avg. loss: 1.7990, Accuracy: 4747/12000 (40%)\n",
            "\n",
            "Validation set accuracy:  39.55833435058594\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.761684\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.966709\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 1.789698\n",
            "\n",
            "Validation set: Avg. loss: 1.7707, Accuracy: 4937/12000 (41%)\n",
            "\n",
            "Validation set accuracy:  41.141666412353516\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.533392\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.747250\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 1.882357\n",
            "\n",
            "Validation set: Avg. loss: 1.8167, Accuracy: 4613/12000 (38%)\n",
            "\n",
            "Validation set accuracy:  38.44166564941406\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.729989\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.702099\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 1.680454\n",
            "\n",
            "Validation set: Avg. loss: 1.7899, Accuracy: 4641/12000 (39%)\n",
            "\n",
            "Validation set accuracy:  38.67499923706055\n",
            "Train Epoch: 1 [0/38000 (0%)]\tLoss: 1.571982\n",
            "Train Epoch: 1 [12800/38000 (34%)]\tLoss: 1.697794\n",
            "Train Epoch: 1 [25600/38000 (67%)]\tLoss: 1.591802\n",
            "\n",
            "Validation set: Avg. loss: 1.7363, Accuracy: 4932/12000 (41%)\n",
            "\n",
            "Validation set accuracy:  41.099998474121094\n",
            "Best lambda for adam:  0.005\n",
            "Best learning rate for adam:  0.001\n",
            "Best lambda for sgd :  0.005\n",
            "Best learning rate for sgd:  0.001\n",
            "182.13516902800006\n"
          ]
        }
      ],
      "source": [
        "def run_on_dataset(dataset_name, filename):\n",
        "    if dataset_name == \"MNIST\":\n",
        "        min_thres = 0.82\n",
        "        max_thres = 0.92\n",
        "\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "        min_thres = 0.28\n",
        "        max_thres = 0.38\n",
        "\n",
        "    correct_predict, accuracy, run_time = run(logistic_regression, dataset_name, filename)\n",
        "\n",
        "    score = compute_score(accuracy, min_thres, max_thres)\n",
        "    result = OrderedDict(correct_predict=correct_predict,\n",
        "                         accuracy=accuracy, score=score,\n",
        "                         run_time=run_time)\n",
        "    return result, score\n",
        "\n",
        "\n",
        "def main():\n",
        "    filenames = { \"MNIST\": \"predictions_mnist_YourName_IDNumber.txt\", \"CIFAR10\": \"predictions_cifar10_YourName_IDNumber.txt\"}\n",
        "    result_all = OrderedDict()\n",
        "    score_weights = [0.5, 0.5]\n",
        "    scores = []\n",
        "    for dataset_name in [\"MNIST\",\"CIFAR10\"]:\n",
        "        result_all[dataset_name], this_score = run_on_dataset(dataset_name, filenames[dataset_name])\n",
        "        scores.append(this_score)\n",
        "    total_score = [score * weight for score, weight in zip(scores, score_weights)]\n",
        "    total_score = np.asarray(total_score).sum().item()\n",
        "    result_all['total_score'] = total_score\n",
        "    with open('result.txt', 'w') as f:\n",
        "        f.writelines(pformat(result_all, indent=4))\n",
        "    print(\"\\nResult:\\n\", pformat(result_all, indent=4))\n",
        "\n",
        "\n",
        "main()\n",
        "start = timeit.default_timer()\n",
        "tune_hyper_parameter()\n",
        "stop = timeit.default_timer()\n",
        "run_time = stop - start\n",
        "print(run_time)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}