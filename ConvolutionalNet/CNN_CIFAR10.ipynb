{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDVDq4R4cQqN"
      },
      "source": [
        "Import and setup some auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qHPwL1QYcQqU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "import timeit\n",
        "from collections import OrderedDict\n",
        "from pprint import pformat\n",
        "from torch.utils.data.sampler import *\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from google.colab import drive\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "def compute_score(acc, min_thres, max_thres):\n",
        "    if acc <= min_thres:\n",
        "        base_score = 0.0\n",
        "    elif acc >= max_thres:\n",
        "        base_score = 100.0\n",
        "    else:\n",
        "        base_score = float(acc - min_thres) / (max_thres - min_thres) \\\n",
        "                     * 100\n",
        "    return base_score\n",
        "\n",
        "\n",
        "def run(algorithm, dataset_name, filename):\n",
        "    predicted_test_labels, gt_labels, run_time, parameters_count = algorithm(dataset_name)\n",
        "    if predicted_test_labels is None or gt_labels is None:\n",
        "      return (0, 0, 0)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for label, prediction in zip(gt_labels, predicted_test_labels):\n",
        "      total += label.size(0)\n",
        "      correct += (prediction.cpu().numpy() == label.cpu().numpy()).sum().item()   # assuming your model runs on GPU\n",
        "      \n",
        "    accuracy = float(correct) / total\n",
        "    \n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "    return (correct, accuracy, run_time, parameters_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwEPmfvc-rHH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3VfLcA4cQqx"
      },
      "source": [
        "Your implementation starts here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "17Mjmw05cQq0"
      },
      "outputs": [],
      "source": [
        "# Part[1] TODO: Cifar-10 dataloading\n",
        "import torchvision\n",
        "from torch.utils.data import random_split\n",
        "def load_data(dataset_name, device, config):\n",
        "    \"\"\"\n",
        "    loads cifar-10 dataset using torchvision, take the last 5k of the training data to be validation data\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    loads cifar-10 dataset using torchvision, take the last 5k of the training data to be validation data\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size_train = 20\n",
        "    batch_size_test = 1\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    CIFAR10_training = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                    download=True, transform=transform)\n",
        "    CIFAR10_test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                                    download=True, transform=transform)\n",
        "\n",
        "    # create a training and a validation set\n",
        "    CIFAR10_training_set, CIFAR10_validation_set = random_split(\n",
        "        CIFAR10_training, [45000, 5000])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataloader = torch.utils.data.DataLoader(CIFAR10_training_set,\n",
        "                                               batch_size=batch_size_train,\n",
        "                                               shuffle=True, num_workers=2)\n",
        "    valid_dataloader = torch.utils.data.DataLoader(CIFAR10_validation_set,\n",
        "                                                    batch_size=batch_size_train,\n",
        "                                                    shuffle=True, num_workers=2)\n",
        "    test_dataloader = torch.utils.data.DataLoader(CIFAR10_test_set,\n",
        "                                              batch_size=batch_size_test,\n",
        "                                              shuffle=False, num_workers=2)\n",
        "\n",
        "        \n",
        "    return train_dataloader, valid_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "f2PKEVDSeeJ9"
      },
      "outputs": [],
      "source": [
        "# Part [2] TODO: Main model definition + any utilities such as weight initialization\n",
        "# Reference https://shonit2096.medium.com/cnn-on-cifar10-data-set-using-pytorch-34be87e09844\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # convolutional layer\n",
        "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
        "        # max pooling layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.upSampling = nn.UpsamplingNearest2d(scale_factor=2)\n",
        "        # fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 10)\n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(p=.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # add sequence of convolutional and max pooling layers\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.upSampling(x)\n",
        "        x = self.pool(x)\n",
        "        # flattening\n",
        "        x = x.view(-1, 32 * 4 * 4)\n",
        "        # fully connected layers\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Q3GcXjw5emv0"
      },
      "outputs": [],
      "source": [
        "# Part [3] TODO : Main trainig + validation, returns a trained model\n",
        "import torch.optim as optim\n",
        "def train(train_dataloader, valid_dataloader, device, config):\n",
        "    \n",
        "    n_epochs = 60\n",
        "    learning_rate = 0.01\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def trainA(epoch):\n",
        "      model.train()\n",
        "      for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)+ 0.005 * torch.sum(\n",
        "                torch.sum(torch.pow(list(model.parameters())[0], 2)), 0)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    def validation(epoch):\n",
        "      model.eval()\n",
        "      validation_loss = 0\n",
        "      correct = 0\n",
        "      with torch.no_grad():\n",
        "        for data, target in valid_dataloader:\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          output = model(data)\n",
        "          validation_loss += criterion(output, target).item() + 0.005 * torch.sum(\n",
        "                torch.sum(torch.pow(list(model.parameters())[0], 2)), 0)\n",
        "          pred = output.data.max(1, keepdim=True)[1]\n",
        "          correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "      validation_loss /= len(valid_dataloader.dataset)\n",
        "      print('Train Epoch: {} Validation set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        epoch, validation_loss, correct, len(valid_dataloader.dataset),\n",
        "        100. * correct / len(valid_dataloader.dataset)))\n",
        "    \n",
        "    validation(0)\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        trainA(epoch)\n",
        "        validation(epoch)\n",
        "\n",
        "    return model\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U9dIUkH6e5T2"
      },
      "outputs": [],
      "source": [
        "# Part [4] TODO: Testing + paramater setting\n",
        "# TODO: Testing + paramater setting\n",
        "\n",
        "def save_model_colab_for_submission(model):  # if you are running on colab\n",
        "  drive.mount('/content/gdrive/', force_remount=True)\n",
        "  \n",
        "  torch.save(model.to(torch.device(\"cpu\")), '/content/gdrive/My Drive/model.pt') # you will find the model in your home drive\n",
        "  \n",
        "\n",
        "def save_model_local_for_submission(model):  # if you are running on your local machine\n",
        "  torch.save(model.to(torch.device(\"cpu\")), 'model.pt')\n",
        "  \n",
        "def test(model, test_dataloader, device):\n",
        "  test_predictions = []\n",
        "  true_labels = []\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  count = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_dataloader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      output = model(data)\n",
        "      test_loss += criterion(output, target).item() + 0.005 * torch.sum(\n",
        "                torch.sum(torch.pow(list(model.parameters())[0], 2)), 0)\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      if count == 0:\n",
        "        preds = pred\n",
        "        gts = target.data.view_as(pred)\n",
        "      else:\n",
        "        preds = torch.cat((preds, pred), 0)\n",
        "        gts = torch.cat((gts, target.data.view_as(pred)), 0)\n",
        "      count += 1\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "      test_loss /= len(test_dataloader.dataset)\n",
        "  \n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_dataloader.dataset),\n",
        "    100. * correct / len(test_dataloader.dataset)))\n",
        "  \n",
        "  test_predictions = preds\n",
        "\n",
        "  true_labels = gts\n",
        "\n",
        "  return test_predictions, true_labels\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params+=params\n",
        "    # uncomment below codes for your debugging\n",
        "    #print(table)\n",
        "    #print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "def run_NN(dataset_name):\n",
        "    # set parameters cifar10\n",
        "  config = {\n",
        "        'lr': 0,\n",
        "        'num_epochs': 0,\n",
        "        'batch_size': 0,\n",
        "        'num_classes': 0,\n",
        "        'regular_constant': 0,\n",
        "        'transforms': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) }\n",
        "    \n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  \n",
        "  train_dataloader, valid_dataloader, test_dataloader = load_data(dataset_name, device, config)\n",
        "  \n",
        "  model = train(train_dataloader, valid_dataloader, device, config)\n",
        "  parameters_count = count_parameters(model)\n",
        "\n",
        "  device = torch.device(\"cpu\")\n",
        "  start_time = timeit.default_timer()\n",
        "  assert test_dataloader.batch_size == 1, 'Error: You should use use batch size = 1 for the test loader.'\n",
        "  preds, labels = test(model.to(device), test_dataloader, device)\n",
        "  end_time = timeit.default_timer()\n",
        "  \n",
        "\n",
        "  test_time = (end_time - start_time)\n",
        "  print(\"Total run time of testing the model: \", test_time , \" seconds.\")\n",
        "  \n",
        "  save_model_colab_for_submission(model)\n",
        "  \n",
        "  return preds, labels, test_time, parameters_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNNgL7C7cQq-"
      },
      "source": [
        "Main loop. Run time and total score will be shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf9iL8S_cQrB",
        "outputId": "e295e446-6160-4c5d-be58-4435f409df6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Train Epoch: 0 Validation set: Avg. loss: 0.1164, Accuracy: 472/5000 (9%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Don't edit this cell\n",
        "def run_on_dataset(dataset_name, filename):\n",
        "    min_thres = 0.55\n",
        "    max_thres = 0.65\n",
        "\n",
        "    correct_predict, accuracy, run_time, parameters_count = run(run_NN, dataset_name, filename)\n",
        "\n",
        "    score = compute_score(accuracy, min_thres, max_thres)\n",
        "    if parameters_count > 50000:\n",
        "      score = max(0, score - 25)\n",
        "    result = OrderedDict(\n",
        "                  score=score,\n",
        "                  correct_predict=correct_predict,\n",
        "                  accuracy=accuracy,\n",
        "                  run_time=run_time,\n",
        "                  parameters_count=parameters_count)\n",
        "    return result, score\n",
        "\n",
        "\n",
        "def main():\n",
        "    filenames = { \"CIFAR10\": \"predictions_cifar10_YourName_IDNumber.txt\"}\n",
        "    result_all = OrderedDict()\n",
        "    scores = []\n",
        "    for dataset_name in [\"CIFAR10\"]:\n",
        "        result_all, this_score = run_on_dataset(dataset_name, filenames[dataset_name])\n",
        "    with open('result.txt', 'w') as f:\n",
        "        f.writelines(pformat(result_all, indent=4))\n",
        "    print(\"\\nResult:\\n\", pformat(result_all, indent=4))\n",
        "\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
